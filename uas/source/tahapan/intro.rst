************
Introduction
************ 

Perkembangan teknologi informasi yang semakin pesat diiringi juga semakin meningkatnya serangan cyber seperti  serangan malware, scanning, brute force, ddos maupun perusakan terhadap data-data pribadi yang menyebabkan kerugian besar bagi pemiliknya. Untuk mengatasi permasalahan tersebut tentunya diperlukan sistem untuk mendeteksi serangan yang masuk ke dalam jaringan seperti honeypot. Honeypot merupakan sebuah sistem yang bekerja untuk memantau, mendeteksi serta mengumpulkan informasi penyerangan dan sengaja dijadikan untuk tujuan diserang dan di eksploitasi. Multiple Honeypot memungkinkan pemasangan beberapa sensor honeypot untuk berjalan pada satu server. Secara umum, honeypot dibagi menjadi tiga tingkatan yaitu, low interaction, medium interaction dan high interaction. Semakin tinggi tingkat interaksi pada honeypot, maka semakin besar data yang ditangkap dan semakin besar juga resiko yang diterima. Seiring dengan sulitnya menganalisis log yang dihasilkan oleh honeypot, maka dibutuhkan alat visualisasi untuk mempermudah dalam menganalisis log honeypot.


Pada sebelumnya untuk dapat melihat log yang muncul hanya berupa huruf dan angka, namun kini dengan adanya ELK stack untuk membaca atau melihat log dapat lebih mudah daripada sebelumnya. Hasil log pada honeypot divisualisasikan menggunakan ELK stack, di mana ELK stack ini adalah kombinasi tools berbasis open source yaitu elasticsearch, logstash dan kibana. Hasil visualisasi yang tersimpan pada ELK stack sangat berguna bagi administrator untuk melakukan identifikasi masalah pada server. Proses pengiriman log terjadi ketika serangan honeypot akan mencatat serangan tersebut dan menyimpannya di dalam log .JSON. Data log dari masing-masing honeypot akan dikirim menggunakan plug in filebeat. Log akan diolah ELK server dan kemudian akan divisualisasikan pada web browser.

Dionaea merupakan salah satu honeypot dengan tingkat interaksi menengah yang digunakan untuk menjebak dan mendeteksi serangan. Honeypot ini juga meniru beberapa protokol seperti FTP, HTTP, HTTPS, MySQL, dan lain-lain.

Skema Elasticsearch, Logstash, dan Kibana
#############################################

ELK terdiri dari 3 komponen open source yaitu Elasticsearch, Logstash, dan Kibana yang dapat dieksekusi pada Virtual Hardware Environment. Elasticsearch merupakan mesin pencari berbasis Apache Lucene. Logstash adalah komponen pengumpulan data dinamis untuk mengumpulkan log yang diperlukan dan mengirimkan log tersebut ke komponen Elasticsearch setelah mentrasformasikannya ke format JSON. Kibana merupakan komponen untuk memvisualisasikan berbagai tipe grfik, tabel, peta, dsb.

Elastic Stack digunakan di banyak kasus seperti log operasional dan analisis metrik. Memastikan data dapat dibawa secara terukur, tahan lama, dan aman ke Elasticsearch merupakan hal yang sangat penting terutama untuk lingkup yang kritikal. Tujuan utama dokumen tersebut adalah untuk mensoroti pola arsitektur yang paling umum untuk Logstash dan bagaimana mengefektifkan skala pertumbuhan dalam membangun ELK. Pada dokumen ini diberikan skenario ELK efektif yang dapat digunakan dengan berbagai sumber.

.. figure:: ../../images/elk.jpg
    :width: 600 px
    :align: center
    :height: 400 px
    :alt: alternate text
    :figclass: align-center

    Sumber: (https://www.elastic.co/guide/en/logstash/current/deploying-and-scaling.html)

Berdasarkan gambar tersebut merupakan rancangan ELK yang efektif untuk digunakan oleh administrator dimana rancangan tersebut bersifat fleksibel jika dikombinasikan dengan berbagai layanan open source lainnya.

Log Files
*********

Sebuah sistem atau aplikasi yang bekerja pada sebuah perangkat, hampir selalu memiliki log files. Log files adalah informasi berbentuk teks dalam suatu dokumen yang diperoleh dari suatu sistem digital dan memuat berbagai informasi penting mengenai kondisi suatu sistem atau produk, hingga masalah yang ada pada sistem tersebut. Suatu log files yang memberikan informasi penting tentang aplikasi atau sistem berupa data log memang tidak dirancang untuk dapat dipahami melalui pola pikir manusia secara umum, bahkan jika suatu log files dapat dipahami, akan sulit dalam menangkap secara spesifik inti dari hasil keluaran suatu log files.

Elasticsearch
*************

Elasticsearch adalah aplikasi open source dibuat menggunakan bahasa pemrograman Java yang berfungsi sebagai alat pencarian, penyimpanan dan analisis log, dibangun berdasarkan konsep kerja sistem pencarian Apache Lucene. Elasticsearch merupakan databases berbasis documented oriented storage atau NoSQL database. Metode penyimpanan berorientasi dokumen ini sangat berbeda jauh dengan metode tradisional, yaitu table oriented storage seperti database MySQL dan Oracle, NoSQL database menyimpan data dalam bentuk format JSON (JavaScript Object Notation) document. Pencarian data dengan NoSQL sangat luar biasa cepat disebabkan setiap field terindeks secara otomatis. Elasticsearch compatible dengan berbeagai macam sistem operasi seperti Linux dan Windows.

Elasticsearch menjadi sebuah mesin pencari dan analisis untuk semua jenis data. Di Elasticsearch sendiri akan terjadi suatu proses yang dinamakan Data Ingestion. Proses Data Ingestion ini merupakan proses dimana data mentah diolah melalui tahapan penguraian, normalisasi, dan diperkaya dengan data tambahan sebelum dilakukan proses indexing. Proses indexing lah yang menjadi bukti keberhasilan Elasticsearch dalam melakukan pencarian data dengan cepat. Selain itu, Elasticsearch merupakan alat yang bersifat scalable, yang berarti dapat dilakukan penyesuaian tergantung dari kebutuhan sistem.

Logstash
********

Logstash adalah aplikasi server side data processing berbasis open-source untuk keperluan pengambilan data, dengan kemampuan pipelining secara realtime. Logstash dikembangkan dari satu pengembang yang sama dengan Elasticsearch dan Kibana, yaitu Elastic. Co. Logstash dapat secara dinamis mengabungkan dari beberapa sumber berbeda dan menormalkan kembali data-data yang telah dikumpulkan dan mengarahkan data tersebut ke suatu lokasi tertentu dalam sistem. Logstash menggunakan metode pipelining dalam mengumpulkan data log kemudian meneruskan ke beberapa node-node tertentu secara paralel karena Logstash menggunakan metode real time pipeline processing. Pipeline processing adalah suatu cara memproses data secara paralel dengan membagi proses tersebut dalam sebuah conceptual pipe sehingga hasil keluaran dari suatu proses tersebut menjadi nilai input baru bagi proses lainnya. Proses seperti ini dapat mempercepat kerja suatu sistem serta meningkatkan stabilitas sistem dalam bekerja.

Kibana
******

Kibana adalah aplikasi analisis dan visualisasi berbasis open-source, dirancang untuk dapat bekerja bersamaan dengan Elasticsearch. Kibana dapat mengolah data dalam jumlah yang besar agar mudah dianalisis dan dipahami. Kibana memberikan fitur penyajian data bentuk diagram, tabel, dan kerangka dalam penerapan data analisis tingkat lanjut. Dengan fitur browser-based, memudahkan dalam pembuatan dashboard secara cepat dengan menyesuaikan secara realtime dari query Elasticsearch.

Filebeat
********

Filebeat adalah plug in Logstash yang bertugas sebagai agen pada server sumber untuk mengirim data ke ELK Stack. Filebeat menggantikan plug in Logstash yang lama yaitu Logstash forwarder atau lumberjack. Dalam ELK Stack, log dari beberapa server aplikasi dikirimkan melalui shipper Logstash ke induk Logstash. Pada Logstash dilakukan parsing data yang kemudian dikirim menuju cluster Elasticsearch. Data yang tersimpan pada Elasticsearch digunakan oleh Kibana untuk menampilkan visualisasi.

